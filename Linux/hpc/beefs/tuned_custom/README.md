# Documenta√ß√£o e Plano de A√ß√£o ‚Äì BeeGFS Storage em HDD (workload misto)

---

## 1. Contexto

Os servidores BeeGFS-storage no seu cluster HPC usam **HDDs rotacionais** como backend de dados.
O workload √© **misto**:

* Arquivos grandes (datasets massivos, checkpoints, streaming).
* Arquivos pequenos (datasets de Machine Learning, metadados).

### Riscos principais

* HDDs s√£o eficientes em streaming, mas sofrem em I/O aleat√≥rio.
* Sem otimiza√ß√£o, o kernel pode acumular filas muito longas ‚Üí **lat√™ncia alta**.
* Perfis do `tuned` podem sobrescrever tunings manuais ‚Üí √© preciso **customizar**.

---

## 2. Ajustes aplicados

### 2.1 Perfil `tuned.conf` final (`beegfs-storage-hdd-mixed`)

üìÇ `/etc/tuned/beegfs-storage-hdd-mixed/tuned.conf`

```ini
# Perfil Tuned customizado para servidor de armazenamento BeeGFS
# Otimizado para: Discos R√≠gidos (HDDs)
# Carga de trabalho: Mista (arquivos grandes e pequenos, t√≠pico de ML/HPC)
# Foco: Maximizar throughput e garantir lat√™ncia consistente.

[main]
# Descri√ß√£o amig√°vel do perfil, vis√≠vel com 'tuned-adm list'.
summary=BeeGFS Storage em HDD (workload misto: grandes + pequenos arquivos ML)

# Herda todas as configura√ß√µes do perfil 'throughput-performance' como base.
# As configura√ß√µes abaixo ir√£o sobrescrever ou adicionar novas regras.
include=throughput-performance

[sysctl]
# ---- Rede ----
# Otimiza√ß√µes para redes de alta velocidade (10GbE+) e alto volume de conex√µes.

# Define o buffer m√°ximo de recebimento de socket (256 MiB) para evitar perda de pacotes em redes de alta velocidade.
net.core.rmem_max=268435456
# Define o buffer m√°ximo de envio de socket (256 MiB) para otimizar a transmiss√£o de grandes volumes de dados.
net.core.wmem_max=268435456
# Aumenta a fila de pacotes recebidos antes do processamento pelo kernel para lidar com rajadas de tr√°fego.
net.core.netdev_max_backlog=250000
# Define os buffers de recebimento TCP (min, default, max), permitindo que conex√µes usem at√© 256 MiB.
net.ipv4.tcp_rmem=4096 87380 268435456
# Define os buffers de envio TCP (min, default, max), permitindo que conex√µes usem at√© 256 MiB.
net.ipv4.tcp_wmem=4096 65536 268435456
# Habilita a descoberta autom√°tica do MTU ideal no caminho de rede para evitar fragmenta√ß√£o de pacotes.
net.ipv4.tcp_mtu_probing=1
# Altera o algoritmo de controle de congestionamento para HTCP, mais agressivo e ideal para redes de alta performance.
net.ipv4.tcp_congestion_control=htcp
# Aumenta a fila de conex√µes pendentes para lidar com um n√∫mero massivo de clientes simult√¢neos.
net.core.somaxconn=65535
# Aumenta a mem√≥ria para dados auxiliares de sockets, suportando opera√ß√µes de rede intensivas.
net.core.optmem_max=25165824

# ---- Mem√≥ria ----
# Otimiza√ß√µes para manter dados na RAM e evitar pausas de I/O.

# Reduz drasticamente o uso da parti√ß√£o swap (valor 1 de 100), for√ßando o sistema a manter os dados na RAM (muito mais r√°pida).
vm.swappiness=1
# Define que processos que geram dados s√£o for√ßados a escrever no disco quando 10% da mem√≥ria est√° "suja".
vm.dirty_ratio=10
# Inicia a escrita de dados "sujos" em segundo plano quando 5% da mem√≥ria est√° ocupada, evitando "I/O storms" (longas pausas de I/O).
vm.dirty_background_ratio=5
# For√ßa o kernel a manter um m√≠nimo de 256 MiB de RAM livre para estabilidade sob carga pesada.
vm.min_free_kbytes=262144

# ---- HDDs: workload misto ----
# Otimiza√ß√µes espec√≠ficas para o subsistema de I/O com discos mec√¢nicos.

# Define o escalonador de I/O para 'mq-deadline', ideal para HDDs por minimizar o movimento das cabe√ßas e dar previsibilidade.
block/sd*/queue/scheduler=mq-deadline
# Aumenta a profundidade da fila de I/O, permitindo que o escalonador otimize mais requisi√ß√µes simultaneamente.
block/sd*/queue/nr_requests=256
# Configura a leitura adiantada (read-ahead) para 512 KB, acelerando a leitura de arquivos sequenciais.
block/sd*/queue/read_ahead_kb=512

[bootloader]
# ATEN√á√ÉO: As altera√ß√µes nesta se√ß√£o exigem uma REINICIALIZA√á√ÉO para serem aplicadas.

# Adiciona os seguintes par√¢metros na linha de comando de inicializa√ß√£o do kernel:
# - transparent_hugepage=never: Desabilita Huge Pages para evitar picos de lat√™ncia e garantir performance consistente.
# - intel_idle.max_cstate=1 / processor.max_cstate=1: Desabilita estados profundos de economia de energia da CPU para minimizar a lat√™ncia de resposta.
cmdline=transparent_hugepage=never intel_idle.max_cstate=1 processor.max_cstate=1
```

---

### 2.2 Pontos-chave do tuning

* **Rede**: buffers grandes, congestion control `htcp`, backlog alto.
* **Mem√≥ria**: baixo uso de swap, flush de dirty pages mais agressivo.
* **Discos (HDD)**:

  * `mq-deadline` ‚Üí melhor para fairness em discos mec√¢nicos.
  * `nr_requests=256` ‚Üí fila moderada, evita sobrecarga.
  * `read_ahead_kb=512` ‚Üí equil√≠brio entre arquivos grandes e pequenos.
* **Boot params**:

  * `transparent_hugepage=never` ‚Üí evita jitter.
  * `intel_idle.max_cstate=1` e `processor.max_cstate=1` ‚Üí reduz lat√™ncia de wake-up da CPU.

---

## 3. Checklist de Valida√ß√£o

1. Ativar servi√ßo e perfil:

   ```bash
   systemctl enable --now tuned
   tuned-adm profile beegfs-storage-hdd-mixed
   tuned-adm active
   ```

2. Conferir discos:

   ```bash
   cat /sys/block/sd*/queue/scheduler
   cat /sys/block/sd*/queue/nr_requests
   cat /sys/block/sd*/queue/read_ahead_kb
   ```

3. Conferir rede:

   ```bash
   sysctl net.core.rmem_max
   sysctl net.core.wmem_max
   sysctl net.ipv4.tcp_congestion_control
   ```

4. Conferir mem√≥ria:

   ```bash
   sysctl vm.swappiness
   sysctl vm.dirty_ratio
   sysctl vm.dirty_background_ratio
   ```

5. Conferir boot params:

   ```bash
   cat /proc/cmdline
   ```

---

## 4. Plano de Testes com **IOR 4.0.0 + mpirun**

### 4.1 Ambiente

* **IOR 4.0.0** compilado com MPI.
* Executar em pelo menos 2 n√≥s clientes acessando o BeeGFS-storage.
* Diret√≥rio de teste: `/mnt/beegfs/test/`.

### 4.2 Comandos de benchmark

üîπ Arquivos grandes (streaming sequencial, 1 MB block, 16 MB transfer):

```bash
mpirun -np 8 numactl --cpunodebind=0 --membind=0 \
  ior -w -r -o /mnt/beegfs/test/bigfile \
      -t 1m -b 16m -F -k
```

üîπ Arquivos pequenos (4 KB blocos, simula√ß√£o de ML dataset):

```bash
mpirun -np 8 numactl --cpunodebind=0 --membind=0 \
  ior -w -r -o /mnt/beegfs/test/smallfiles \
      -t 4k -b 64k -F -k
```

üîπ Workload misto (128 KB blocos, equil√≠brio):

```bash
mpirun -np 8 numactl --cpunodebind=0 --membind=0 \
  ior -w -r -o /mnt/beegfs/test/mixed \
      -t 128k -b 4m -F -k
```

* `-F` ‚Üí cada processo escreve seu pr√≥prio arquivo.
* `-k` ‚Üí manter os arquivos ap√≥s o teste (para inspe√ß√£o).

---

## 5. Plano de A√ß√£o ‚Äì Implanta√ß√£o controlada

1. **Ambiente de Teste (PoC)**

   * Escolher 1 servidor BeeGFS-storage com HDD.
   * Ativar perfil `beegfs-storage-hdd-mixed`.
   * Rodar os benchmarks IOR acima.
   * Registrar m√©tricas (IOPS, throughput, lat√™ncia).

2. **Compara√ß√£o com baseline**

   * Reverter para `virtual-guest` ou `throughput-performance`.
   * Rodar os mesmos testes IOR.
   * Comparar os resultados.

3. **Valida√ß√£o com workload real**

   * Rodar aplica√ß√µes de ML/DL reais (treino leve com TensorFlow/PyTorch) usando BeeGFS.
   * Verificar impacto em lat√™ncia e throughput.

4. **Escalonar**

   * Se os ganhos forem consistentes, aplicar o perfil em todos os storage targets HDD.
   * Manter monitoramento ativo (`beegfs-ctl --getentryinfo`, `beegfs-ctl --storagepools`) e m√©tricas de I/O.

---

# üéØ Conclus√£o

* O perfil `beegfs-storage-hdd-mixed` traz equil√≠brio entre grandes arquivos (streaming) e pequenos arquivos (ML).
* Testes com **IOR 4.0.0 + mpirun** garantem valida√ß√£o objetiva antes da produ√ß√£o.
* A implanta√ß√£o deve seguir **PoC ‚Üí Compara√ß√£o ‚Üí Workload real ‚Üí Escalonamento**.

---
